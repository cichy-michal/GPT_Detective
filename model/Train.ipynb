{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczność zbioru treningowego: 1233\n",
      "Liczność zbioru testowego: 138\n",
      "Epoch 1/20\n",
      "39/39 - 56s - 1s/step - accuracy: 0.4728 - loss: 0.6781\n",
      "Epoch 2/20\n",
      "39/39 - 43s - 1s/step - accuracy: 0.5069 - loss: 0.6780\n",
      "Epoch 3/20\n",
      "39/39 - 46s - 1s/step - accuracy: 0.5053 - loss: 0.6786\n",
      "Epoch 4/20\n",
      "39/39 - 47s - 1s/step - accuracy: 0.5142 - loss: 0.6774\n",
      "Epoch 5/20\n",
      "39/39 - 48s - 1s/step - accuracy: 0.5215 - loss: 0.6751\n",
      "Epoch 6/20\n",
      "39/39 - 46s - 1s/step - accuracy: 0.5580 - loss: 0.6711\n",
      "Epoch 7/20\n",
      "39/39 - 44s - 1s/step - accuracy: 0.5839 - loss: 0.6321\n",
      "Epoch 8/20\n",
      "39/39 - 45s - 1s/step - accuracy: 0.6699 - loss: 0.5873\n",
      "Epoch 9/20\n",
      "39/39 - 46s - 1s/step - accuracy: 0.7802 - loss: 0.5494\n",
      "Epoch 10/20\n",
      "39/39 - 45s - 1s/step - accuracy: 0.6732 - loss: 0.6099\n",
      "Epoch 11/20\n",
      "39/39 - 47s - 1s/step - accuracy: 0.5069 - loss: 0.6817\n",
      "Epoch 12/20\n",
      "39/39 - 49s - 1s/step - accuracy: 0.4964 - loss: 0.6771\n",
      "Epoch 13/20\n",
      "39/39 - 48s - 1s/step - accuracy: 0.5028 - loss: 0.6692\n",
      "Epoch 14/20\n",
      "39/39 - 49s - 1s/step - accuracy: 0.5296 - loss: 0.6614\n",
      "Epoch 15/20\n",
      "39/39 - 49s - 1s/step - accuracy: 0.5280 - loss: 0.6577\n",
      "Epoch 16/20\n",
      "39/39 - 48s - 1s/step - accuracy: 0.5345 - loss: 0.6673\n",
      "Epoch 17/20\n",
      "39/39 - 48s - 1s/step - accuracy: 0.4996 - loss: 0.6612\n",
      "Epoch 18/20\n",
      "39/39 - 47s - 1s/step - accuracy: 0.6050 - loss: 0.6334\n",
      "Epoch 19/20\n",
      "39/39 - 43s - 1s/step - accuracy: 0.6472 - loss: 0.5862\n",
      "Epoch 20/20\n",
      "39/39 - 46s - 1s/step - accuracy: 0.7875 - loss: 0.5029\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 286ms/step - accuracy: 0.7928 - loss: 0.5911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5091013312339783, 0.7898550629615784]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def replace_polish_characters(text):\n",
    "    replacements = {\n",
    "        'ą': 'a', 'ć': 'c', 'ę': 'e', 'ł': 'l', 'ń': 'n', 'ó': 'o', 'ś': 's', 'ź': 'z', 'ż': 'z',\n",
    "        'Ą': 'A', 'Ć': 'C', 'Ę': 'E', 'Ł': 'L', 'Ń': 'N', 'Ó': 'O', 'Ś': 'S', 'Ź': 'Z', 'Ż': 'Z'\n",
    "    }\n",
    "    for polish_char, replacement in replacements.items():\n",
    "        text = tf.strings.regex_replace(text, polish_char, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = replace_polish_characters(text)\n",
    "    text = tf.strings.regex_replace(text, r'[^a-zA-Z0-9\\s]', '')\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "ds = tf.data.TextLineDataset(\"Dataset2.csv\")\n",
    "\n",
    "ds_size = sum(1 for _ in ds) # dataset size\n",
    "\n",
    "ds = ds.shuffle(buffer_size=ds_size, reshuffle_each_iteration=False)\n",
    "\n",
    "train_size = int(0.90 * ds_size)\n",
    "\n",
    "ds_train = ds.take(train_size)\n",
    "ds_test = ds.skip(train_size)\n",
    "\n",
    "print(\"Liczność zbioru treningowego:\", sum(1 for _ in ds_train))\n",
    "print(\"Liczność zbioru testowego:\", sum(1 for _ in ds_test))\n",
    "\n",
    "tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "def build_vocabulary(ds_train, treshhold=2):\n",
    "    frequencies = {}\n",
    "    vocabulary = set()\n",
    "    vocabulary.update([\"sostoken\"])\n",
    "    vocabulary.update([\"eostoken\"])\n",
    "    \n",
    "    for line in ds_train.skip(1):\n",
    "        split_line = tf.strings.split(line, \";\", maxsplit=2)\n",
    "        article = split_line[2]\n",
    "        clean_article = clean_text(tf.strings.lower(article))\n",
    "        tokenized_text = tokenizer.tokenize(tf.get_static_value(clean_article))\n",
    "        \n",
    "        for word in tokenized_text:\n",
    "            if word not in frequencies:\n",
    "                frequencies[word] = 1\n",
    "            else:\n",
    "                frequencies[word] += 1\n",
    "            \n",
    "            if frequencies[word] == treshhold:\n",
    "                vocabulary.update(tokenized_text)\n",
    "        \n",
    "    return vocabulary\n",
    "\n",
    "# Create and save vocabulary\n",
    "#vocabulary = build_vocabulary(ds_train)\n",
    "\n",
    "#with open(\"vocabulary.json\", \"w\") as vocab_file:\n",
    "#    json.dump(list(vocabulary), vocab_file)\n",
    "    \n",
    "with open(\"vocabulary.json\", \"r\") as vocab_file:\n",
    "    vocabulary = json.load(vocab_file)\n",
    "\n",
    "encoder = tfds.deprecated.text.TokenTextEncoder(\n",
    "    list(vocabulary), oov_token=\"<UNK>\", lowercase=True, tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "def encode_map_fn(line):\n",
    "    split_line = tf.strings.split(line, \";\", maxsplit=2)\n",
    "    label_str = split_line[1] # AI, Human\n",
    "    article = \"sostoken \" + split_line[2] + \" eostoken\"\n",
    "    clean_article = clean_text(tf.strings.lower(article))\n",
    "    label = tf.cond(tf.equal(label_str, \"ai\"), lambda: tf.constant(1), lambda: tf.constant(0))\n",
    "    \n",
    "    encoded_text, label = tf.py_function(\n",
    "        my_encoder, \n",
    "        inp=[clean_article, label], \n",
    "        Tout=(tf.int64, tf.int32)\n",
    "    )\n",
    "    \n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    \n",
    "    return encoded_text, label\n",
    "\n",
    "def my_encoder(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy().decode('utf-8'))\n",
    "    return encoded_text, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ds_train = ds_train.map(encode_map_fn, num_parallel_calls=AUTOTUNE).cache()\n",
    "ds_train = ds_train.shuffle(300)\n",
    "ds_train = ds_train.padded_batch(32, padded_shapes=([None],()))\n",
    "\n",
    "ds_test = ds_test.map(encode_map_fn)\n",
    "ds_test = ds_test.padded_batch(32, padded_shapes=([None], ()))\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Embedding(input_dim=len(vocabulary)+2, output_dim=32),\n",
    "        layers.Bidirectional(LSTM(256, return_sequences=True)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        #layers.Dropout(0.5),\n",
    "        #layers.Bidirectional(LSTM(64)),\n",
    "       # layers.Bidirectional(LSTM(32, return_sequences=True)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(ds_train, epochs=20, verbose=2) \n",
    "model.evaluate(ds_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, 'GPDT(94).keras') #zapis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding {'name': 'embedding', 'trainable': True, 'dtype': 'float32', 'input_dim': 20675, 'output_dim': 32, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}\n",
      "lstm {'name': 'lstm', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 256, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}\n",
      "global_average_pooling1d {'name': 'global_average_pooling1d', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'keepdims': False}\n",
      "dropout {'name': 'dropout', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}\n",
      "dense {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "dropout_1 {'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}\n",
      "dense_1 {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "dense_2 {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('model_testowy(90%).keras')\n",
    "\n",
    "# Iterate through the layers and print their configuration\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.get_config())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
